---
layout: page
title: Tutorials
permalink: /tutorials/
---

Tutorials with equation derivations, explanations and code for various machine learning and statistics topics.  These tutorials are intended as introductions for current topics in machine learning, with a focus on derivation of results and accompanying code to demonstrate results. All tutorials can be found on [[GitHub](https://github.com/aky4wn/Tutorials)]



## Variational Inference

This tutorial provides an introduction to some important topics in variational inference.  Variational inference is a way to perform posterior inference via optimization, rather than sampling.  The Expectation-Maximization (EM) algorithm is also covered in this tutorial.

The main working example for this tutorial is the Gaussian Mixture Model.  The EM algorithm, variational inference and Markov Chain Monte Carlo methods are all applied to learn the parameters in the Gaussian mixture model.  Inference procedures for each method are derived and discussed. Finally, Latent Dirichlet Allocation is presented as a slightly more advanced example of variational inference.

This tutorial can be found on [[GitHub](https://github.com/aky4wn/Tutorials/tree/master/Variational_Inference)].


## Coming Soon: Hidden Markov Models and Kalman Filters 

Maximum likelihood, Markov Chain Monte Carlo and variational inference methods for sequential models, focusing on hidden Markov models and Kalman Filters.
